---
title: "Qualitative Prediction of Weightlifting Exercises"
author: "Krishanu Maitra"
date: "21 September 2018"
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Executive Summary

In this project, we prepare a model to predict the manner in which certain weightlifting excercises are performed, i.e. correctly or incorrectly. This is based on data collected from  accelerometers in wearable devices on the belt, forearm, arm, and dumbell of 6 participants while they performed barbell lifts correctly and incorrectly in 5 different ways.

The data for this project is taken from http://groupware.les.inf.puc-rio.br/har.

The training data for this project are available here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
The test data are available here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The outcome of each way the exercises are performed is given in the __*classe*__ column of these datasets.

We have downloaded the datasets in our working directory for preparing this prediction model.

We will use the following R libraries for the purpose of our project:

```{r,echo=TRUE,warning=FALSE,message=FALSE}
library(caret)
library(e1071)
library(dplyr)
library(randomForest)
```

## Loading the data

```{r,echo=TRUE}
training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")
```

## Data Cleaning

We first load and understand the the training and the testing datasets.

```{r,echo=FALSE}
dim(training)
```

```{r,echo=FALSE}
names(training)
```
We find that there are 159 parameters and 1 outcome variable, classe, in the dataset.

Further, we find out which columns are blank or have NA values and how many:

```{r,echo=TRUE}
a <- apply(training, MARGIN = 2, FUN = function(x)(sum(is.na(x) | x == "")))
a[a>0]
```

We find that out of the 160 columns, `r length(a[a>0])` columns have `r a[1]` NA or blank values out of the `r dim(training)[1]` observations. We also note that the outcome column, classe, is not among these columns, i.e. it does not have any NA or blank value. We therefore conclude that these columns do not have any significance as predictors and remove these columns from the training dataset.

```{r,echo=TRUE}
training <- subset(training, select = setdiff(names(training), names(a[a>0])))
```

## Exploratory Analysis

We now look at the training dataset:

```{r,echo=TRUE}
str(training)
```

We can see that the dataset contains a column X, which is the observation serial number, user_name, various sensor measurements and an outcome (classe) for each observation. The new_window and num_window columns refer to the sampling of continuous sensor data to extract discrete features.

We find out the distribution of the observations based on outcomes for each user:

```{r,echo=TRUE}
with(training, table(user_name, classe))
```

We also note that the observations seem to be independent and uncorrelated to each other, as each measurement is from a different point, e.g. belt, arm, forearm and dumbbell and in an independent direction, x, y or z.

Next, we explore the relevant columns of the testing dataset:

```{r,echo=TRUE}
chk <- subset(testing, select = setdiff(names(testing), names(a[a>0])))
str(chk)
```

The X column in the training dataset is only a serial number for the observations. We also note that the testing dataset does not have any user details, nor does it have the classe column containing the actual outcomes. 

## Validation dataset

Since the testing dataset does not have the outcome column, classe, we use random sampling to split the training dataset and create the validation dataset. we will use 80% of the observations in the training dataset for training our prediction models and 20% of the observations for validating them.

The training dataset has different observations for six users classified into one of the classe levels. Therefore, in order to get a balanced validation dataset, we will split each user and classe combination into 80% for training and 20% for validation and append those splits to get our final training and validation datasets.

```{r,echo=TRUE}
users <- unique(training$user_name)
classes <- unique(training$classe)
modTrain <- NULL
validation <- NULL
for(i in users) {
    for(j in classes) {
        subdata <- subset(training, user_name==i & classe==j)
        inTrain <- createDataPartition(y=subdata$X, p=0.8, list=FALSE)
        modTrain <- rbind(modTrain, subdata[inTrain,])
        validation <- rbind(validation, subdata[-inTrain,])
    }   
}
```

Since the testing dataset does not have any user details, our prediction model should be able to predict the outcome based on the set of sensor data irrespective of the user. The observations are independent events and not part of a time series, hence the columns raw_timestamp_part_1 and raw_timestamp_part_2 are irrelevant as well. Since we already have the extracted features, the num_window and new_window columns can also be ignored. Therefore, we decide to remove these columns from the training and the cross-validation datasets before fitting our prediction model.

```{r,echo=TRUE}
modTrain <- subset(modTrain, select =  setdiff(names(modTrain),c("X","user_name", "raw_timestamp_part_1", "raw_timestamp_part_2","cvtd_timestamp", "num_window", "new_window")))
validation <- subset(validation, select =  setdiff(names(validation),c("X","user_name", "raw_timestamp_part_1", "raw_timestamp_part_2","cvtd_timestamp", "num_window", "new_window")))
```

## Creating the Prediction Model

The given problem is a classification problem. We try to fit a number of models below and evaluate their prediction performance on the training and the cross-validation sets.

We set the random seed as follows:

```{r,echo=TRUE}
set.seed(32323)
```

### Recursive Partitioning with rpart

#### rpart Approach 1: Random Sampling for cross-validation

We fit the model as follows and check the correctness of its predictions:

```{r,echo=TRUE}
modFit <- train(classe ~ ., method="rpart", data=modTrain)
modFit
confusionMatrix(modFit)
```

We now check how the model performs the classification in the validation set.

```{r,echo=TRUE}
pred <- predict(modFit, newdata=validation)
confusionMatrix(pred,validation$classe)
plot(modFit)
```

We can see that the accuracy of this model is about 50% in the training as well as the cross-validation set. Hence we change the cross-validation approach.

#### rpart Approach 2: Using K-fold Cross-validation

We again try fitting a model using rpart, but using K-fold cross-validation with 10 folds.

```{r,echo=TRUE}
numFolds <- trainControl(method = "cv", number = 10)
modFit <- train(classe ~ ., method="rpart", trControl = numFolds, data=modTrain)
modFit
confusionMatrix(modFit)
```

Model performance on the validation dataset:

```{r,echo=TRUE}
pred <- predict(modFit, newdata=validation)
confusionMatrix(pred,validation$classe)
plot(modFit)
```

We find that even after using K-fold for cross-validation, there is not much improvement in the model accuracy.

Hence we now try the Random Forest classification model.

### Random Forest 

First we check the significant features using rfcv() function, using the default values of cv.fold=5, step=0.5 and scale="log":

```{r,echo=TRUE}
numCols <- length(names(modTrain))
modFit <- rfcv(trainx = modTrain[,-numCols], trainy = modTrain[,numCols])
```
```{r,echo=TRUE}
plot(modFit$n.var, modFit$error.cv, log = "x", type = "o", lwd = 1.5,         xlab = "Number of variables selected", ylab = "Cross-Validation Error")
```

Thus, We can get a low cross-validation error by using only the 13 most important variables. To identify those variables, we run a generic Random Forest model and check the what the 13 most important variables are:
```{r,echo=TRUE}
n <- 13
modFit <- randomForest(classe ~ ., data=modTrain)
varImpPlot(modFit, n.var = n)
```

Next, we train another model using only the most important variables:

```{r,echo=TRUE}
vars <- varImp(modFit)
impCols <- rownames(vars)[order(vars, decreasing = TRUE)][1:n]
modFit <- randomForest(x = modTrain[,impCols], y = modTrain$classe)
```

Then we check the predictions made by this model on the validation set and compare them with the actual predictions:

```{r,echo=TRUE}
pred <- predict(modFit, newdata=validation)
confusionMatrix(pred,validation$classe)
plot(modFit)
```

We can see that the accuracy of the model in the validation set is about 99% and the 95% CI lies between 0.98 and 0.99.

Finally, we use the model to make predictions in the testing dataset and write the results out to a csv file to submit for evaluation:

```{r,echo=TRUE}
modTest <- subset(testing, select = setdiff(names(testing), names(a[a>0])))
modTest <- subset(modTest, select =  setdiff(names(modTest),c("X","user_name", "raw_timestamp_part_1", "raw_timestamp_part_2","cvtd_timestamp", "num_window", "new_window")))
modTest <- mutate(modTest, classe = "A")
testPred <- predict(modFit, newdata=modTest)
testing <- mutate(testing, classe = testPred)
write.csv(testing, file = "newtesting.csv")
```

## Conclusion
We conclude that qualitative prediction of weightlifting exercises can be successfully accomplished using a model that implements Random Forest with cross-validation.

## Reference

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.